{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc00398",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Function\n",
    "import torch.tensor as Tensor\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52988e63",
   "metadata": {},
   "source": [
    "## Neuron Model (core forward pass equation)\n",
    "\\\n",
    "We use a simple piecewise linear postsynaptinc potential based spiking neuron model which has a very low computational cost\\\n",
    "The membrane potential $v_i(t)$ of neuron $i$ at time $t$ is the weighted summation of the PL-PSPs of its aferent neurons:\n",
    "$$\n",
    "v_i(t) = \\sum_{j\\in J}w_{ij}\\epsilon(t-t_j)\n",
    "$$\n",
    "where, $w_{ij}$ is the synaptic weight connecting the presynaptic neuron $j$ to the neuron $i$ and $t_j$is the spike time of neuron $j$. $\\epsilon(t-t_j)$ is the kernel of the PL-PSP function. It's described by the following equation:\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\epsilon(t-t_j) = \\begin{cases}\n",
    "\\frac{t-t_j}{\\tau_1}, & \\text{if } t_j \\le t < t_j+\\tau_1; \\\\\n",
    "\\frac{t_j+\\tau_{1}+\\tau_{2}-t}{\\tau_{2}}, & \\text{if } t_j + \\tau_1 \\le t < t_j + \\tau_1 + \\tau_2.\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6eb9154",
   "metadata": {},
   "source": [
    "### Target calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5697624c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_firing_time_output(output: Tensor, \n",
    "                              tmax: int, label: Tensor, gamma: int, device, dtype)->Tensor:\n",
    "    \n",
    "    factory_kwargs = {'device':device, 'dtype':dtype}\n",
    "    tmp = output\n",
    "    tau_min = torch.min(output, axis=1, keepdim=True)[0]\n",
    "    labeled = torch.zeros_like(tmp, **factory_kwargs) + tau_min - gamma\n",
    "    labeled = torch.where(labeled<0, torch.full_like(tmp, 0), labeled)\n",
    "    tau_max = torch.max(output, axis=1, keepdim=True)[0]\n",
    "    unlabeled = torch.zeros_like(tmp, **factory_kwargs) + tau_max + gamma\n",
    "    unlabeled = torch.where(unlabeled>tmax, torch.full_like(tmp, tmax) , unlabeled)\n",
    "    target = torch.zeros_like(output, requires_grad=False)\n",
    "    batch_size = target.shape[0]\n",
    "    label1 = torch.arange(batch_size, **factory_kwargs).unsqueeze(1).tolist()\n",
    "    label2 = label.unsqueeze(1).int().tolist()\n",
    "    target[label1, label2] = 1\n",
    "    target = torch.where(target==1, labeled, unlabeled)\n",
    "    \n",
    "    return target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20190457",
   "metadata": {},
   "source": [
    "### Fully connected layer\n",
    "\n",
    "(1) forward: neuron model\\\n",
    "(2) backward:\\\n",
    "the loss function of each layer $l$ is calculated independently by the following equation:\n",
    "$$ E^l = \\sum_{j}E^l_j = \\sum_j\\frac{1}{2}(e_j^l)^2 $$\n",
    "where, $e_j^l$ is the temporal error function for the postsynaptic neuron $j$ obtained by substracting the desired and the actual firing times($T_j^l$ and $t_j^l$, respectively) of the neuron $j$ in the $l^{th}$ layer:\n",
    "$$ e_j^l = \\frac{T_j^l-t_j^l}{T_{max}}$$\n",
    "and for our gradient of loss function:\n",
    "$$\n",
    "\\begin{equation}\n",
    " \\Delta w_{l}^{ji} =\n",
    " \\begin{cases} \n",
    " -\\eta \\frac{\\partial E_j^l}{\\partial t_j^l}\\frac{\\partial t_j^l}{\\partial v_j^l(t)}\\frac{\\partial v_j^l(t)}{\\partial w_{ji}^l}, & \\text{if } t_i^{l-1} \\le t_j^l \\\\\n",
    " 0, & \\text{otherwise}.\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f380a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# neuron model\n",
    "class Spiking_linear(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, firing_time: Tensor, weight: Tensor, tau1: int, tau2: int, tmax: int, threshold: int, beta: int, device, dtype) -> Tensor:\n",
    "        \"\"\"\n",
    "        :param ctx: same as self\n",
    "        :param firing_time: batch_size*(i-1)-th neuron\n",
    "        :param weight: (i)-th neuron * (i-1)-th neuron\n",
    "        :param tau1: int\n",
    "        :param tau2: int\n",
    "        :param tmax: maimum time interval to fire\n",
    "        :param threshold: threshold of firing\n",
    "        :param beta: calculate for target firing time in middle layers\n",
    "        :return:\n",
    "            :param output: batch_size * i-th neuron\n",
    "        \"\"\"\n",
    "        \n",
    "        batch_size = firing_time.shape[0]\n",
    "        neuron_num = firing_time.shape[1]\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        \n",
    "        real_firing = torch.where(firing_time > tmax, torch.full_like(firing_time, tmax), firing_time)\n",
    "        \n",
    "        # spread to 0-1 matrix\n",
    "        spike01 = torch.zeros((batch_size, neuron_num, tmax+1+tau1+tau2), **factory_kwargs).float()\n",
    "        label1 = torch.arange(batch_size, **factory_kwargs).unsqueeze(1).unsqueeze(2).tolist()\n",
    "        label2 = torch.meshgrid(torch.arange(batch_size, **factory_kwargs), torch.arange(neuron_num, **factory_kwargs))[1].unsqueeze(2).tolist()\n",
    "        # print(real_firing)\n",
    "        # print(real_firing.unsqueeze(2).shape, torch.arange(tau1+tau2+1, **factory_kwargs).unsqueeze(0).unsqueeze(0).shape)\n",
    "        label3 = real_firing.unsqueeze(2).int() + torch.arange(tau1+tau2+1, **factory_kwargs).unsqueeze(0).unsqueeze(0)\n",
    "        \n",
    "        a = torch.linspace(0,1,steps=tau1+1, **factory_kwargs)\n",
    "        b = torch.linspace(1,0,steps=tau2+1, **factory_kwargs)\n",
    "        c = torch.cat((a,b[1:]), 0)\n",
    "        spike01[label1, label2, label3] = c\n",
    "        # for i in range(tau1+tau2+1):\n",
    "        #    spike01[label1, label2, (label3+i).tolist()] = c[i]\n",
    "        epsilon = spike01[:,:,:tmax+1]\n",
    "        # spike01 shape: batch_size, i-1 th neuron, tmax+1\n",
    "        # weight shape: i-th neuron i-1 th neuron\n",
    "         \n",
    "        # get voltage of next level neuron\n",
    "        Voltage = torch.matmul(weight, epsilon).cumsum(dim = 2)\n",
    "        # print('why voltage cannot grad?')\n",
    "        \n",
    "        # get the firing_time of next level neuron\n",
    "        Spike = Voltage > threshold\n",
    "        Spike[:,:,-1] = 1\n",
    "        output = torch.argmax(torch.eq(Spike.cumsum(axis=2).cumsum(axis=2), 1).int(), axis=2)\n",
    "        output = output.float()\n",
    "        # output = torch.where((output > tmax), torch.full_like(output, tmax), output).float()\n",
    "        \n",
    "        # firing_time.register_hook(lambda grad: print('firing_time grad: ', grad))\n",
    "        # weight.register_hook(lambda grad: print('weight grad: ', grad))\n",
    "        \n",
    "        ctx.save_for_backward(output, firing_time, weight)\n",
    "        \n",
    "        ctx.tau1, ctx.tau2, ctx.tmax, ctx.threshold, ctx.beta = tau1, tau2, tmax, threshold, beta\n",
    "        \n",
    "        return output.requires_grad_(True)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_outputs: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        :param ctx: same as self\n",
    "        :param grad_outputs: postsynaptic loss\n",
    "        :return:\n",
    "            grad\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        output: batch_size * i-th neuron\n",
    "        firing_time: batch_size * (i-1)-th neuron\n",
    "        weight: i-th neuron * (i-1)-th neuron\n",
    "        \"\"\"\n",
    "        output, firing_time, weight = ctx.saved_tensors\n",
    "        tau1, tau2, tmax, threshold, beta = ctx.tau1, ctx.tau2, ctx.tmax, ctx.threshold, ctx.beta\n",
    "        # print(\"beta type:\", type(beta))\n",
    "        # print(\"tau1 type:\", type(tau1))\n",
    "        assert type(beta) == int\n",
    "        \n",
    "        # get hasfired: shape(batch_size, (i-1)-th neuron, i-th neuron)\n",
    "        hasfired = (firing_time.transpose(0,1) < output.transpose(0,1).unsqueeze(1)).transpose(0,2).contiguous()\n",
    "        \n",
    "        # hasfired.shape = batch_size, i-1th neuron, ith neuron\n",
    "        # output.shape = batch_size, ith neuron\n",
    "        # firing_time.shape = batch_size, i-1th neuron\n",
    "        # grad_ouputs.shape = batch_size, ith neuron\n",
    "        \n",
    "        # calculate grad_weight: grad_weight(ij) = grad_output(j) * (- tj/threshold) * epsilon(tj - ti)(has fired)\n",
    "        epsilon = (output.transpose(0,1) - firing_time.transpose(0,1).unsqueeze(1)).float()\n",
    "        epsilon = torch.where(epsilon<0, torch.full_like(epsilon, 0), epsilon)\n",
    "        epsilon = torch.where((epsilon>=0)&(epsilon<tau1), epsilon/tau1, epsilon)\n",
    "        epsilon = torch.where((epsilon>=tau1)&(epsilon<tau1+tau2), (tau1+tau2-epsilon)/tau2, epsilon)\n",
    "        epsilon = torch.where((epsilon>=tau1+tau2), torch.full_like(epsilon,0), epsilon).float().transpose(0,2).contiguous()\n",
    "        # epsilon.shape: batch_size*i-th neuron*(i-1)-th neuron\n",
    "\n",
    "        grad_weight = torch.sum(((grad_outputs * -(output.float()/threshold)).unsqueeze(2) * epsilon), axis = 0)\n",
    "        \n",
    "        # calculate grad_input(for the former layer to calculate)\n",
    "        tmp = beta * grad_outputs * -(output.float()/threshold)\n",
    "        dv = (output.transpose(0,1) - firing_time.transpose(0,1).unsqueeze(1)).transpose(0,1)\n",
    "        tochange1 = (dv >= 0) & (dv < tau1)\n",
    "        tochange2 = (dv >= tau1) & (dv < tau1 + tau2)\n",
    "        dvdt = - tochange1.int() * (weight / tau1).unsqueeze(2) + tochange2.int() * (weight / tau2).unsqueeze(2)\n",
    "        dvdt = dvdt.transpose(0,2).contiguous()\n",
    "        dvdt = dvdt.transpose(1,2).contiguous()\n",
    "        deltat = (dvdt*tmp.unsqueeze(2)).transpose(1,2).contiguous()\n",
    "        # print(tochange1.shape, tochange2.shape, dvdt.shape, tmp.shape)\n",
    "        # deltat = torch.matmul(dvdt, tmp.unsqueeze(2)).squeeze(2)\n",
    "        grad_input = deltat/(tmax*tmax)*hasfired.float()\n",
    "        grad_input = grad_input.sum(axis = 2)\n",
    "        \n",
    "        return grad_input, grad_weight, None, None, None, None, None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f95c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer model\n",
    "class Spike_linear(nn.Module):\n",
    "    \n",
    "    __constants__ = ['in_features', 'out_features', 'tau1', 'tau2', 'tmax', 'threshold', 'beta']\n",
    "    \n",
    "    def __init__(self, in_features:int, out_features:int, \n",
    "                 tau1:int, tau2:int, tmax:int, threshold:int, beta:int, device, dtype) -> None:\n",
    "        self.factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        self.device = device\n",
    "        self.dtype = dtype\n",
    "        super(Spike_linear, self).__init__()\n",
    "        # self.weight = nn.Parameter(torch.empty((out_features, in_features), **self.factory_kwargs))\n",
    "        self.tmax, self.tau1, self.tau2 = tmax, tau1, tau2\n",
    "        self.threshold, self.beta = threshold, beta\n",
    "        self.weight = nn.Parameter(0.5 * torch.rand((out_features, in_features), **self.factory_kwargs))\n",
    "        \n",
    "    def reset_parameters(self, upperbound:int, lowerbound:int) -> None:\n",
    "        self.weight = nn.Parameter((upperbound - lowerbound) * torch.rand_like(self.weight, **self.factory_kwargs) + lowerbound)\n",
    "        \n",
    "    def forward(self, firing_time: Tensor) -> Tensor:\n",
    "        return Spiking_linear.apply(firing_time, self.weight, self.tau1, self.tau2, self.tmax, self.threshold, self.beta, self.device, self.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7282344",
   "metadata": {},
   "outputs": [],
   "source": [
    "class linear_loss_f(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, target, tmax):\n",
    "        result = torch.sum((input-target)*(input-target)/(tmax*tmax), axis = 1, keepdim = True)/2\n",
    "        ctx.save_for_backward(input, target)\n",
    "        ctx.tmax = tmax\n",
    "        return input.new(result)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, target = ctx.saved_tensors\n",
    "        tmax = ctx.tmax\n",
    "        result = (input - target)/(tmax*tmax)\n",
    "        return grad_output.new(result), None, None\n",
    "    \n",
    "def linear_loss(firing, target, tmax):\n",
    "    return linear_loss_f.apply(firing, target, tmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4daa14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class S4NN(nn.Module):\n",
    "    def __init__(self, input_size: int = 784, hidden_size: int = 400, classes: int=10, tau1: int = 40, tau2: int = 40, tmax: int=256, beta: int=1,  device= 'cpu', dtype = None) -> None:\n",
    "        super(S4NN, self).__init__()\n",
    "        self.layer1 = Spike_linear(input_size, hidden_size, tau1, tau2, tmax, 50, beta, device, dtype)\n",
    "        self.layer1.reset_parameters(0.25, 0)\n",
    "        self.layer2 = Spike_linear(hidden_size, classes, tau1, tau2, tmax, 10, beta, device, dtype)\n",
    "        self.layer2.reset_parameters(0.5, 0)\n",
    "\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        :param input: batch_size, input_size\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return self.layer2(self.layer1(input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b2c2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.MNIST(root='./data/', train=True, transform = transforms.ToTensor(), download = True)\n",
    "test_dataset = datasets.MNIST(root='./data/', train=False, transform = transforms.ToTensor(), download = True)\n",
    "training_batch, testing_batch = 1, 1000\n",
    "train_loader = DataLoader(dataset = train_dataset, batch_size = training_batch, shuffle = True, drop_last = True)\n",
    "test_loader = DataLoader(dataset = test_dataset, batch_size = testing_batch, shuffle = True, drop_last = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858ab65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "input_size, hidden_size, num_class = 10, 6, 5\n",
    "I_max, tmax = 1, 256  # constant for computing spike time from pixel value\n",
    "nepoch = 1000  # n of epochs\n",
    "gamma = 10  # the constant for computing target\n",
    "Dropout = [0, 0]  # didn't realize dropout\n",
    "lr, lamda = 1, 0  # learning rate, L1 regularization\n",
    "tau1, tau2 = 40, 40\n",
    "# device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# model = S4NN(input_size = input_size, hidden_size = hidden_size, classes = num_class, tau1 = tau1, tau2 = tau2, tmax = tmax, beta = 1, device = device, dtype = None)\n",
    "model = final_S4NN(input_size = input_size, hidden_size = hidden_size, classes = num_class, tmax = tmax, beta = 1, device = device, dtype = None)\n",
    "\n",
    "tensorboard_path = './log/figures'\n",
    "writer = SummaryWriter(tensorboard_path)\n",
    "\n",
    "model_path = './log/models'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e7a4e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.set_printoptions(threshold = np.inf)\n",
    "file = open(\"test.txt\", 'w').close()\n",
    "\n",
    "model.train()\n",
    "# optimizer = torch.optim.SGD(params = model.parameters(), lr = lr, weight_decay = lamda)\n",
    "optimizer = torch.optim.SGD([\n",
    "            {'params': model.layer1.parameters(), 'lr': 0.2},\n",
    "            {'params': model.layer2.parameters(), 'lr': 0.2}])\n",
    "\n",
    "\"\"\"print(\"init parameters: \")\n",
    "for parameters in model.parameters():\n",
    "    print(parameters)\n",
    "\"\"\"\n",
    "has_set = False\n",
    "\n",
    "for epoch in range(nepoch):\n",
    "    print('begin epoch {}'.format(epoch + 1))\n",
    "    # print('begin epoch {}'.format(epoch + 1), file = f)\n",
    "    start = time.time()\n",
    "    \n",
    "    right, al = 0, 0\n",
    "    for datas, labels in train_loader:\n",
    "        datas, labels = datas.to(device), labels.to(device)\n",
    "        datas = datas.view(training_batch, -1)\n",
    "        datas = (((I_max - datas)/I_max)*tmax).int().float().requires_grad_()\n",
    "        # print(\"datas = \", datas)\n",
    "        # print(\"labels = \", labels)\n",
    "        # assert 1 == -1\n",
    "\n",
    "        with open(\"test.txt\", \"a\") as file1:\n",
    "            print(\"datas = \", datas, file = file1)\n",
    "\n",
    "        output = model(datas)\n",
    "\n",
    "        target = target_firing_time_output(output = output, tmax = tmax, label = labels, gamma = gamma, device = device, dtype = None).detach()\n",
    "\n",
    "        with open(\"test.txt\", \"a\") as file1:\n",
    "            print('output = ', output, file = file1)\n",
    "            print('target = ', target, file = file1)\n",
    "        # tmp = torch.sum(torch.pow((target - output)/tmax, 2), axis = 1, keepdims = True)/2\n",
    "        # print(tmp, torch.mean(tmp, axis = 1))\n",
    "        # loss = torch.mean(torch.sum(torch.pow((target - output)/(tmax*tmax), 1), axis = 1, keepdims = True))\n",
    "        # print('loss = ',loss, loss.shape)\n",
    "        # assert 1 == 0\n",
    "\n",
    "        loss = linear_loss(output, target, tmax)\n",
    "        # print(\"loss = \", loss)\n",
    "        # loss.retain_grad()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward(torch.ones(loss.size()).to(device))\n",
    "        with open(\"test.txt\", \"a\") as file1:\n",
    "            for name, parms in model.named_parameters():\n",
    "                    print('-->name:', name, file = file1)\n",
    "                    print('-->para:', parms, file = file1)\n",
    "                    print('-->grad_requirs:',parms.requires_grad, file = file1)\n",
    "                    print('-->grad_value:',parms.grad, file = file1)\n",
    "                    print(\"=====================================\", file = file1)\n",
    "            print(\"after step:\", file = file1)      \n",
    "        optimizer.step()\n",
    "        with open(\"test.txt\", \"a\") as file1:\n",
    "            for name, parms in model.named_parameters():\n",
    "                    print('-->name:', name, file = file1)\n",
    "                    print('-->para:', parms, file = file1)\n",
    "                    print('-->grad_requirs:',parms.requires_grad, file = file1)\n",
    "                    print('-->grad_value:',parms.grad, file = file1)\n",
    "                    print(\"=====================================\", file = file1)\n",
    "                    \n",
    "        right = right + (torch.min(output, axis=1, keepdim=False)[1]==labels).sum()\n",
    "        al = al + training_batch\n",
    "        \n",
    "    end = time.time()\n",
    "    print(\"training accuracy: {}\".format(right/al))\n",
    "    print('Epoch {} finished in {} seconds (100 datas training)'.format(epoch+1, end-start))\n",
    "        \n",
    "    \n",
    "    #print(\"after training epoch {}, parameters:\".format(epoch+1))\n",
    "    #for parameters in model.parameters():\n",
    "    #    print(parameters)\n",
    "    # print('Epoch {} finished in {} seconds (60000 datas training)'.format(epoch+1, end-start), file = f)\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        all_correct, all_samples = 0, 0\n",
    "        for datas, labels in test_loader:\n",
    "            datas, labels = datas.to(device), labels.to(device)\n",
    "            datas = datas.view(testing_batch, -1)\n",
    "            datas = (((I_max - datas)/I_max)*tmax).int().float()\n",
    "            output = model(datas)\n",
    "            predict = torch.argmin(output, dim=1)\n",
    "            correct = torch.sum(predict == labels)\n",
    "            all_correct += correct\n",
    "            all_samples += testing_batch\n",
    "        testing_loss = 1 - all_correct/all_samples\n",
    "        print('After epoch {}, the testing accuracy is {}%'.format(epoch + 1, 100 * all_correct / all_samples))\n",
    "        # print('After epoch{}, the testing accuracy is {}%'.format(epoch + 1, 100 * all_correct / all_samples), file = f)\n",
    "\n",
    "        all_correct, all_samples = 0, 0\n",
    "        count, max_count = 0, 10000\n",
    "        for datas, labels in train_loader:\n",
    "            count = count + 1\n",
    "            datas, labels = datas.to(device), labels.to(device)\n",
    "            datas = datas.view(training_batch, -1)\n",
    "            datas = (((I_max - datas) / I_max) * tmax).int().float()\n",
    "            output = model(datas)\n",
    "            predict = torch.argmin(output, dim=1)\n",
    "            correct = torch.sum(predict == labels)\n",
    "            all_correct += correct\n",
    "            all_samples += training_batch\n",
    "            if count == max_count:\n",
    "                break\n",
    "        training_loss = 1 - all_correct/all_samples\n",
    "        print(\"After epoch {}, the training accuracy is {}%\".format(epoch + 1, 100 * all_correct / all_samples))\n",
    "        # print(\"After epoch {}, the training accuracy is {}%\".format(epoch + 1, 100 * all_correct / all_samples), file=f)\n",
    "\n",
    "        torch.save(model.state_dict(), model_path+'/model_after_epoch{}.pkl'.format(epoch+1))\n",
    "        # writer.add_scalar(tag = 'loss/testing_loss', scalar_value=testing_loss, global_step=epoch+1)\n",
    "        # writer.add_scalar(tag = 'loss/training_loss', scalar_value=training_loss, global_step=epoch+1)\n",
    "\n",
    "        if (testing_loss < 0.35) and (has_set==False):\n",
    "            has_set = True\n",
    "            set_learning_rate = (optimizer, 0.01)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
